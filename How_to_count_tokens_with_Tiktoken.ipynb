{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elichen/oai-playground/blob/main/How_to_count_tokens_with_Tiktoken.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g1y0MuUF9TSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233e9e10-4c77-4308-9c83-97722724ca74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade tiktoken\n",
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RJHBZXem9D43"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=userdata.get('openai-key'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "def download_file(url, save_dir=\".\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    file_name = os.path.basename(url)\n",
        "    try:\n",
        "        with open(os.path.join(save_dir, file_name), 'wb') as f:\n",
        "            f.write(requests.get(url).content)\n",
        "        print(f\"Downloaded {file_name}\")\n",
        "    except:\n",
        "        print(f\"Failed to download {file_name}\")"
      ],
      "metadata": {
        "id": "gBwJ0hZfxORJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "bNsvl2foZ3LS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"tiktoken is great!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OFZusajacWX",
        "outputId": "91f27b0d-ad2f-4c72-b9ae-3be267c53d53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[83, 1609, 5963, 374, 2294, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZp1x2pyc-NE",
        "outputId": "0729b4af-8d51-461d-9168-0a81c656fb02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_1ybARTqdBQS",
        "outputId": "104bd868-fa79-4c1a-8178-deaf32ea48e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tiktoken is great!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttLDQvaJdC5Y",
        "outputId": "4b943f10-2728-44ac-c1e7-40c16a092f35"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b't', b'ik', b'token', b' is', b' great', b'!']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_encodings(example_string: str) -> None:\n",
        "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
        "    # print the example string\n",
        "    print(f'\\nExample string: \"{example_string}\"')\n",
        "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
        "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\"]:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        token_integers = encoding.encode(example_string)\n",
        "        num_tokens = len(token_integers)\n",
        "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
        "        print()\n",
        "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
        "        print(f\"token integers: {token_integers}\")\n",
        "        print(f\"token bytes: {token_bytes}\")\n",
        "\n",
        "compare_encodings(\"antidisestablishmentarianism\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L50_rwh_dGSr",
        "outputId": "4d8daf92-b758-41e4-8537-4dd81d530916"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"antidisestablishmentarianism\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "cl100k_base: 6 tokens\n",
            "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
            "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"2 + 2 = 4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ufs1_1vdKeD",
        "outputId": "4d35de5b-d3b7-4ef2-ecba-c7dbcc058b11"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"2 + 2 = 4\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "cl100k_base: 7 tokens\n",
            "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"お誕生日おめでとう\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNhYaIdfdNYz",
        "outputId": "3c921325-a97e-4119-c5d7-2ed6e34f16ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"お誕生日おめでとう\"\n",
            "\n",
            "r50k_base: 14 tokens\n",
            "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
            "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
            "\n",
            "p50k_base: 14 tokens\n",
            "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
            "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
            "\n",
            "cl100k_base: 9 tokens\n",
            "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n",
            "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0613\",\n",
        "        \"gpt-3.5-turbo-16k-0613\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "        }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens\n"
      ],
      "metadata": {
        "id": "Ul-kFZ1PdO2s"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's verify the function above matches the OpenAI API response\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"New synergies will help drive top-line growth.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Things working well together will increase revenue.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for model in [\n",
        "    \"gpt-3.5-turbo\",\n",
        "    \"gpt-4-0314\",\n",
        "    \"gpt-4-0613\",\n",
        "    \"gpt-4\",\n",
        "    ]:\n",
        "    print(model)\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
        "    # example token count from the OpenAI API\n",
        "    response = client.chat.completions.create(model=model,\n",
        "    messages=example_messages,\n",
        "    temperature=0,\n",
        "    max_tokens=1)\n",
        "    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64eIi_QXdShH",
        "outputId": "332c1b0e-6de4-4451-92bf-5939fe4164d8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo\n",
            "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "129 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4-0314\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "129 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4-0613\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "129 prompt tokens counted by the OpenAI API.\n",
            "\n",
            "gpt-4\n",
            "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "129 prompt tokens counted by the OpenAI API.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5UMWFbWydU-3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhoDiFFoK5I+bQS2vZnL8/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}